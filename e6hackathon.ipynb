{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13038904,"sourceType":"datasetVersion","datasetId":8256416},{"sourceId":13038972,"sourceType":"datasetVersion","datasetId":8256469}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers accelerate bitsandbytes huggingface_hub -U --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T12:15:33.626428Z","iopub.execute_input":"2025-09-13T12:15:33.627096Z","iopub.status.idle":"2025-09-13T12:17:23.024730Z","shell.execute_reply.started":"2025-09-13T12:15:33.627071Z","shell.execute_reply":"2025-09-13T12:17:23.023978Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.9/374.9 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.5/561.5 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T12:17:28.453061Z","iopub.execute_input":"2025-09-13T12:17:28.453640Z","iopub.status.idle":"2025-09-13T12:17:28.543396Z","shell.execute_reply.started":"2025-09-13T12:17:28.453611Z","shell.execute_reply":"2025-09-13T12:17:28.542893Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from huggingface_hub import login\nlogin(secret_value_0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T12:17:34.266170Z","iopub.execute_input":"2025-09-13T12:17:34.266710Z","iopub.status.idle":"2025-09-13T12:17:34.687941Z","shell.execute_reply.started":"2025-09-13T12:17:34.266688Z","shell.execute_reply":"2025-09-13T12:17:34.687417Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"!pip install sqlglot --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T14:35:41.441178Z","iopub.execute_input":"2025-09-12T14:35:41.441468Z","iopub.status.idle":"2025-09-12T14:35:44.478551Z","shell.execute_reply.started":"2025-09-12T14:35:41.441449Z","shell.execute_reply":"2025-09-12T14:35:44.477346Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import sqlglot\nfrom sqlglot import exp\n\ndef extract_query_features(sql):\n    \"\"\"\n    Extract tables, columns, join types & conditions, and aggregation functions from a SQL query.\n    \"\"\"\n    try:\n        expression = sqlglot.parse_one(sql)\n    except Exception as e:\n        return {\"error\": str(e)}\n\n    tables = set()\n    columns = set()\n    joins = []\n    aggregations = set()\n\n    # --- Tables ---\n    for table in expression.find_all(exp.Table):\n        tables.add(table.name)\n\n    # --- Columns ---\n    for col in expression.find_all(exp.Column):\n        columns.add(col.name)\n\n    # --- Joins and Join Conditions ---\n    for join in expression.find_all(exp.Join):\n        join_type = join.args.get(\"kind\", \"INNER\").upper()  # Default to INNER\n        condition = None\n        # The join expression stores its condition in 'on'\n        if \"on\" in join.args and join.args[\"on\"]:\n            condition = join.args[\"on\"].sql()\n        joins.append({\"type\": join_type, \"condition\": condition})\n\n    # --- Aggregation Functions ---\n    for func in expression.find_all(exp.Func):\n        func_name = func.sql_name().upper()\n        if func_name in {\"SUM\", \"COUNT\", \"AVG\", \"MIN\", \"MAX\"}:\n            aggregations.add(func_name)\n\n    return {\n        \"tables\": list(tables),\n        \"columns\": list(columns),\n        \"joins\": joins,\n        \"aggregations\": list(aggregations)\n    }\n\n# --- Example Usage ---\nqueries = [\n    \"SELECT c.name, SUM(o.amount) FROM customers c JOIN orders o ON c.id=o.customer_id GROUP BY c.name;\",\n    \"SELECT COUNT(*) FROM orders WHERE customer_id = 42;\",\n    \"UPDATE inventory SET stock=stock-1 WHERE product_id=101;\"\n]\n\nfor q in queries:\n    features = extract_query_features(q)\n    print(f\"\\nQuery: {q}\")\n    for k, v in features.items():\n        print(f\"{k.capitalize()}: {v}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T14:45:58.851547Z","iopub.execute_input":"2025-09-12T14:45:58.852139Z","iopub.status.idle":"2025-09-12T14:45:58.862211Z","shell.execute_reply.started":"2025-09-12T14:45:58.852117Z","shell.execute_reply":"2025-09-12T14:45:58.861590Z"}},"outputs":[{"name":"stdout","text":"\nQuery: SELECT c.name, SUM(o.amount) FROM customers c JOIN orders o ON c.id=o.customer_id GROUP BY c.name;\nTables: ['orders', 'customers']\nColumns: ['customer_id', 'name', 'id', 'amount']\nJoins: [{'type': 'INNER', 'condition': 'c.id = o.customer_id'}]\nAggregations: ['SUM']\n\nQuery: SELECT COUNT(*) FROM orders WHERE customer_id = 42;\nTables: ['orders']\nColumns: ['customer_id']\nJoins: []\nAggregations: ['COUNT']\n\nQuery: UPDATE inventory SET stock=stock-1 WHERE product_id=101;\nTables: ['inventory']\nColumns: ['stock', 'product_id']\nJoins: []\nAggregations: []\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Example collected stats (as if from Performance Schema)\nmysql_stats = [\n    {\n        \"sql_text\": \"SELECT * FROM orders WHERE customer_id = 42;\",\n        \"exec_time_ms\": 180,\n        \"rows_sent\": 120,\n        \"rows_examined\": 10000,\n        \"join_type\": \"SIMPLE\",\n        \"index_used\": False\n    },\n    {\n        \"sql_text\": \"SELECT * FROM orders WHERE customer_id = 99;\",\n        \"exec_time_ms\": 200,\n        \"rows_sent\": 100,\n        \"rows_examined\": 9500,\n        \"join_type\": \"SIMPLE\",\n        \"index_used\": False\n    },\n    {\n        \"sql_text\": \"SELECT c.name, SUM(o.amount) FROM customers c JOIN orders o ON c.id=o.customer_id GROUP BY c.name;\",\n        \"exec_time_ms\": 7200,\n        \"rows_sent\": 450,\n        \"rows_examined\": 500000,\n        \"join_type\": \"HASH JOIN\",\n        \"index_used\": True\n    }\n]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T08:55:32.862974Z","iopub.execute_input":"2025-09-13T08:55:32.863331Z","iopub.status.idle":"2025-09-13T08:55:32.868780Z","shell.execute_reply.started":"2025-09-13T08:55:32.863296Z","shell.execute_reply":"2025-09-13T08:55:32.867943Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import re\nfrom collections import defaultdict\n\ndef normalize_query(sql):\n    # Replace literals for grouping\n    sql = re.sub(r\"'[^']*'\", \"'?'\", sql)\n    sql = re.sub(r'\"[^\"]*\"', '\"?\"', sql)\n    sql = re.sub(r'\\b\\d+(\\.\\d+)?\\b', '?', sql)\n    return re.sub(r'\\s+', ' ', sql).strip().lower()\n\n# Group stats by normalized template\ngroups = defaultdict(list)\nfor entry in mysql_stats:\n    template = normalize_query(entry[\"sql_text\"])\n    groups[template].append(entry)\n\n# Aggregate metrics for each group\naggregated = []\nfor template, entries in groups.items():\n    avg_exec_time = sum(e[\"exec_time_ms\"] for e in entries) / len(entries)\n    avg_rows_examined = sum(e[\"rows_examined\"] for e in entries) / len(entries)\n    index_usage_rate = sum(1 for e in entries if e[\"index_used\"]) / len(entries)\n    aggregated.append({\n        \"template\": template,\n        \"frequency\": len(entries),\n        \"avg_exec_time_ms\": avg_exec_time,\n        \"avg_rows_examined\": avg_rows_examined,\n        \"index_usage_rate\": index_usage_rate,\n        \"join_types\": list({e[\"join_type\"] for e in entries})\n    })\n\n# Display results\nfor agg in aggregated:\n    print(f\"\\nTemplate: {agg['template']}\")\n    print(f\"Frequency: {agg['frequency']}\")\n    print(f\"Average Exec Time (ms): {agg['avg_exec_time_ms']:.2f}\")\n    print(f\"Average Rows Examined: {agg['avg_rows_examined']:.0f}\")\n    print(f\"Index Usage Rate: {agg['index_usage_rate']*100:.0f}%\")\n    print(f\"Join Types: {agg['join_types']}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T08:55:36.232386Z","iopub.execute_input":"2025-09-13T08:55:36.232644Z","iopub.status.idle":"2025-09-13T08:55:36.241557Z","shell.execute_reply.started":"2025-09-13T08:55:36.232623Z","shell.execute_reply":"2025-09-13T08:55:36.240571Z"}},"outputs":[{"name":"stdout","text":"\nTemplate: select * from orders where customer_id = ?;\nFrequency: 2\nAverage Exec Time (ms): 190.00\nAverage Rows Examined: 9750\nIndex Usage Rate: 0%\nJoin Types: ['SIMPLE']\n\nTemplate: select c.name, sum(o.amount) from customers c join orders o on c.id=o.customer_id group by c.name;\nFrequency: 1\nAverage Exec Time (ms): 7200.00\nAverage Rows Examined: 500000\nIndex Usage Rate: 100%\nJoin Types: ['HASH JOIN']\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import pandas as pd\n\n# --- Aggregated queries from earlier ---\naggregated_queries = [\n    {\n        \"template\": \"select * from orders where customer_id = ?;\",\n        \"frequency\": 2,\n        \"avg_exec_time_ms\": 190.0,\n        \"avg_rows_examined\": 9750,\n        \"index_usage_rate\": 0.0,\n        \"join_types\": [\"SIMPLE\"]\n    },\n    {\n        \"template\": \"select c.name, sum(o.amount) from customers c join orders o on c.id=o.customer_id group by c.name;\",\n        \"frequency\": 1,\n        \"avg_exec_time_ms\": 7200.0,\n        \"avg_rows_examined\": 500000,\n        \"index_usage_rate\": 1.0,\n        \"join_types\": [\"HASH JOIN\"]\n    },\n    {\n        \"template\": \"update inventory set stock=stock-? where product_id=?;\",\n        \"frequency\": 2,\n        \"avg_exec_time_ms\": 80.0,\n        \"avg_rows_examined\": 50,\n        \"index_usage_rate\": 0.5,\n        \"join_types\": [\"SIMPLE\"]\n    }\n]\n\n# --- Index metadata ---\nindexes_df = pd.DataFrame([\n    {\"TABLE_NAME\": \"orders\",    \"INDEX_NAME\": \"PRIMARY\",       \"COLUMN_NAME\": \"id\",           \"NON_UNIQUE\": 0},\n    {\"TABLE_NAME\": \"orders\",    \"INDEX_NAME\": \"idx_customer\",  \"COLUMN_NAME\": \"customer_id\",  \"NON_UNIQUE\": 1},\n    {\"TABLE_NAME\": \"customers\", \"INDEX_NAME\": \"PRIMARY\",       \"COLUMN_NAME\": \"id\",           \"NON_UNIQUE\": 0},\n    {\"TABLE_NAME\": \"inventory\", \"INDEX_NAME\": \"PRIMARY\",       \"COLUMN_NAME\": \"product_id\",   \"NON_UNIQUE\": 0}\n])\n\n# --- Table sizes and row counts ---\ntable_sizes_df = pd.DataFrame([\n    {\"TABLE_NAME\": \"orders\",    \"row_count\": 500000, \"size_mb\": 820.5},\n    {\"TABLE_NAME\": \"customers\", \"row_count\": 50000,  \"size_mb\": 45.3},\n    {\"TABLE_NAME\": \"inventory\", \"row_count\": 10000,  \"size_mb\": 12.7}\n])\n\n# --- Foreign keys and constraints ---\nconstraints_df = pd.DataFrame([\n    {\"TABLE_NAME\": \"orders\",    \"CONSTRAINT_NAME\": \"fk_orders_customer\",\n     \"CONSTRAINT_TYPE\": \"FOREIGN KEY\", \"COLUMN_NAME\": \"customer_id\",\n     \"REFERENCED_TABLE_NAME\": \"customers\", \"REFERENCED_COLUMN_NAME\": \"id\"},\n    {\"TABLE_NAME\": \"orders\",    \"CONSTRAINT_NAME\": \"PRIMARY\",\n     \"CONSTRAINT_TYPE\": \"PRIMARY KEY\", \"COLUMN_NAME\": \"id\",\n     \"REFERENCED_TABLE_NAME\": None,    \"REFERENCED_COLUMN_NAME\": None},\n    {\"TABLE_NAME\": \"customers\", \"CONSTRAINT_NAME\": \"PRIMARY\",\n     \"CONSTRAINT_TYPE\": \"PRIMARY KEY\", \"COLUMN_NAME\": \"id\",\n     \"REFERENCED_TABLE_NAME\": None,    \"REFERENCED_COLUMN_NAME\": None},\n    {\"TABLE_NAME\": \"inventory\", \"CONSTRAINT_NAME\": \"PRIMARY\",\n     \"CONSTRAINT_TYPE\": \"PRIMARY KEY\", \"COLUMN_NAME\": \"product_id\",\n     \"REFERENCED_TABLE_NAME\": None,    \"REFERENCED_COLUMN_NAME\": None}\n])\n\n# --- Partitioning info (assume none used) ---\npartitions_df = pd.DataFrame(columns=[\n    \"TABLE_NAME\", \"PARTITION_NAME\", \"PARTITION_METHOD\", \"PARTITION_EXPRESSION\"\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T08:55:40.444231Z","iopub.execute_input":"2025-09-13T08:55:40.444467Z","iopub.status.idle":"2025-09-13T08:55:40.780001Z","shell.execute_reply.started":"2025-09-13T08:55:40.444449Z","shell.execute_reply":"2025-09-13T08:55:40.779294Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# --- Assume these DataFrames already exist ---\n# indexes_df: TABLE_NAME, INDEX_NAME, COLUMN_NAME, NON_UNIQUE\n# table_sizes_df: TABLE_NAME, row_count, size_mb\n# constraints_df: TABLE_NAME, CONSTRAINT_NAME, CONSTRAINT_TYPE, COLUMN_NAME, REFERENCED_TABLE_NAME, REFERENCED_COLUMN_NAME\n# partitions_df: TABLE_NAME, PARTITION_NAME, PARTITION_METHOD, PARTITION_EXPRESSION\n# aggregated_queries: output from grouping & aggregating runtime stats earlier\n\n# Build a quick lookup for table sizes and indexes\ntable_size_map = table_sizes_df.set_index('TABLE_NAME')[['row_count', 'size_mb']].to_dict('index')\nindex_map = indexes_df.groupby('TABLE_NAME')['COLUMN_NAME'].apply(list).to_dict()\n\ndef enrich_query_context(aggregated_queries):\n    enriched = []\n    for entry in aggregated_queries:\n        tables = re.findall(r'from\\s+(\\w+)', entry['template']) + re.findall(r'join\\s+(\\w+)', entry['template'])\n        tables = list(set(tables))  # unique tables\n\n        # Gather metadata for each table\n        meta = []\n        for t in tables:\n            meta.append({\n                \"table\": t,\n                \"row_count\": table_size_map.get(t, {}).get('row_count', 'unknown'),\n                \"size_mb\": table_size_map.get(t, {}).get('size_mb', 'unknown'),\n                \"indexes\": index_map.get(t, []),\n                \"has_partitions\": t in partitions_df['TABLE_NAME'].values,\n                \"constraints\": constraints_df[constraints_df['TABLE_NAME'] == t].to_dict('records')\n            })\n\n        enriched.append({\n            **entry,\n            \"tables_meta\": meta\n        })\n    return enriched\n\n# Example usage: create enriched query contexts\nenriched_queries = enrich_query_context(aggregated_queries)\n\n# Print first enriched context\nfrom pprint import pprint\npprint(enriched_queries[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T08:55:48.109977Z","iopub.execute_input":"2025-09-13T08:55:48.110375Z","iopub.status.idle":"2025-09-13T08:55:48.160307Z","shell.execute_reply.started":"2025-09-13T08:55:48.110350Z","shell.execute_reply":"2025-09-13T08:55:48.159734Z"}},"outputs":[{"name":"stdout","text":"{'avg_exec_time_ms': 190.0,\n 'avg_rows_examined': 9750,\n 'frequency': 2,\n 'index_usage_rate': 0.0,\n 'join_types': ['SIMPLE'],\n 'tables_meta': [{'constraints': [{'COLUMN_NAME': 'customer_id',\n                                   'CONSTRAINT_NAME': 'fk_orders_customer',\n                                   'CONSTRAINT_TYPE': 'FOREIGN KEY',\n                                   'REFERENCED_COLUMN_NAME': 'id',\n                                   'REFERENCED_TABLE_NAME': 'customers',\n                                   'TABLE_NAME': 'orders'},\n                                  {'COLUMN_NAME': 'id',\n                                   'CONSTRAINT_NAME': 'PRIMARY',\n                                   'CONSTRAINT_TYPE': 'PRIMARY KEY',\n                                   'REFERENCED_COLUMN_NAME': None,\n                                   'REFERENCED_TABLE_NAME': None,\n                                   'TABLE_NAME': 'orders'}],\n                  'has_partitions': False,\n                  'indexes': ['id', 'customer_id'],\n                  'row_count': 500000,\n                  'size_mb': 820.5,\n                  'table': 'orders'}],\n 'template': 'select * from orders where customer_id = ?;'}\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"def detect_optimization_candidates(enriched_queries,\n                                   slow_time_ms=1000,\n                                   high_freq=2,\n                                   poor_selectivity_ratio=50,\n                                   large_table_mb=500):\n    \"\"\"\n    Detects candidate queries for different optimizations:\n    - High execution time + high frequency\n    - Poor selectivity (rows scanned >> rows returned)\n    - Joins on non-indexed foreign keys\n    - Aggregations over huge tables\n    \"\"\"\n    candidates = {\"slow_and_frequent\": [], \"poor_selectivity\": [],\n                  \"join_no_index\": [], \"heavy_aggregation\": []}\n\n    for q in enriched_queries:\n        # --- High execution time + high frequency ---\n        if q[\"avg_exec_time_ms\"] > slow_time_ms and q[\"frequency\"] >= high_freq:\n            candidates[\"slow_and_frequent\"].append(q)\n\n        # --- Poor selectivity: use avg_rows_examined / rows returned ---\n        rows_returned = max(1, q.get(\"avg_rows_examined\", 1) / poor_selectivity_ratio)  # just for ratio calc\n        # Here we simulate rows_sent = avg_rows_examined/ratio, but in practice you'd have avg_rows_sent\n        # To keep consistent with earlier aggregated_queries, let's fake rows_sent as avg_rows_examined/100\n        avg_rows_sent = max(1, q.get(\"avg_rows_examined\", 1) // 100)\n        if (q[\"avg_rows_examined\"] / avg_rows_sent) > poor_selectivity_ratio:\n            candidates[\"poor_selectivity\"].append(q)\n\n        # --- Joins on non-indexed foreign keys ---\n        for meta in q.get(\"tables_meta\", []):\n            for c in meta.get(\"constraints\", []):\n                if c.get(\"CONSTRAINT_TYPE\") == \"FOREIGN KEY\":\n                    fk_col = c.get(\"COLUMN_NAME\")\n                    # If fk_col not indexed\n                    if fk_col not in meta.get(\"indexes\", []):\n                        candidates[\"join_no_index\"].append(q)\n                        break\n\n        # --- Heavy aggregations on huge tables ---\n        if any(agg in q.get(\"template\", \"\").lower() for agg in [\"sum\", \"count\", \"avg\", \"max\", \"min\"]):\n            if any(meta.get(\"size_mb\", 0) > large_table_mb for meta in q.get(\"tables_meta\", [])):\n                candidates[\"heavy_aggregation\"].append(q)\n\n    return candidates\n\n# --- Example usage ---\ncandidates = detect_optimization_candidates(enriched_queries)\n\n# Print detected candidates\nfor category, items in candidates.items():\n    print(f\"\\n=== {category.upper()} ===\")\n    if not items:\n        print(\"No candidates found.\")\n    for item in items:\n        print(f\"- Template: {item['template']}\")\n        print(f\"  Avg Exec Time (ms): {item['avg_exec_time_ms']}\")\n        print(f\"  Frequency: {item['frequency']}\")\n        print(f\"  Avg Rows Examined: {item['avg_rows_examined']}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T08:55:52.629607Z","iopub.execute_input":"2025-09-13T08:55:52.630265Z","iopub.status.idle":"2025-09-13T08:55:52.638883Z","shell.execute_reply.started":"2025-09-13T08:55:52.630239Z","shell.execute_reply":"2025-09-13T08:55:52.638189Z"}},"outputs":[{"name":"stdout","text":"\n=== SLOW_AND_FREQUENT ===\nNo candidates found.\n\n=== POOR_SELECTIVITY ===\n- Template: select * from orders where customer_id = ?;\n  Avg Exec Time (ms): 190.0\n  Frequency: 2\n  Avg Rows Examined: 9750\n- Template: select c.name, sum(o.amount) from customers c join orders o on c.id=o.customer_id group by c.name;\n  Avg Exec Time (ms): 7200.0\n  Frequency: 1\n  Avg Rows Examined: 500000\n\n=== JOIN_NO_INDEX ===\nNo candidates found.\n\n=== HEAVY_AGGREGATION ===\n- Template: select c.name, sum(o.amount) from customers c join orders o on c.id=o.customer_id group by c.name;\n  Avg Exec Time (ms): 7200.0\n  Frequency: 1\n  Avg Rows Examined: 500000\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"from pprint import pprint\n\ndef build_llm_context(candidates, top_n=3):\n    \"\"\"\n    Build formatted context blocks for LLM recommendations.\n    \"\"\"\n    contexts = []\n    for category, queries in candidates.items():\n        for q in queries[:top_n]:  # limit per category\n            meta_lines = []\n            for meta in q.get(\"tables_meta\", []):\n                constraints = [c['CONSTRAINT_TYPE'] for c in meta.get(\"constraints\", [])]\n                meta_lines.append(\n                    f\"Table '{meta['table']}': {meta.get('row_count','?')} rows, \"\n                    f\"{meta.get('size_mb','?')} MB, Indexes={meta.get('indexes',[])}, \"\n                    f\"Constraints={constraints}, Partitioned={meta.get('has_partitions',False)}\"\n                )\n\n            context_block = f\"\"\"\nCategory: {category}\nQuery Template:\n{q['template']}\n\nPerformance Stats:\n- Frequency: {q['frequency']}\n- Avg Exec Time: {q['avg_exec_time_ms']} ms\n- Avg Rows Examined: {q['avg_rows_examined']}\n\nTable Metadata:\n{chr(10).join(meta_lines)}\n\"\"\"\n            contexts.append(context_block.strip())\n    return contexts\n\n# --- Build contexts ---\ncontexts = build_llm_context(candidates)\n\n# --- Example: Print first context block ---\npprint(contexts[1])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T08:55:57.512047Z","iopub.execute_input":"2025-09-13T08:55:57.512701Z","iopub.status.idle":"2025-09-13T08:55:57.518761Z","shell.execute_reply.started":"2025-09-13T08:55:57.512675Z","shell.execute_reply":"2025-09-13T08:55:57.517968Z"}},"outputs":[{"name":"stdout","text":"('Category: poor_selectivity\\n'\n 'Query Template:\\n'\n 'select c.name, sum(o.amount) from customers c join orders o on '\n 'c.id=o.customer_id group by c.name;\\n'\n '\\n'\n 'Performance Stats:\\n'\n '- Frequency: 1\\n'\n '- Avg Exec Time: 7200.0 ms\\n'\n '- Avg Rows Examined: 500000\\n'\n '\\n'\n 'Table Metadata:\\n'\n \"Table 'orders': 500000 rows, 820.5 MB, Indexes=['id', 'customer_id'], \"\n \"Constraints=['FOREIGN KEY', 'PRIMARY KEY'], Partitioned=False\\n\"\n \"Table 'customers': 50000 rows, 45.3 MB, Indexes=['id'], \"\n \"Constraints=['PRIMARY KEY'], Partitioned=False\")\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\nmodel_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T12:18:04.759154Z","iopub.execute_input":"2025-09-13T12:18:04.759796Z","iopub.status.idle":"2025-09-13T12:21:00.955068Z","shell.execute_reply.started":"2025-09-13T12:18:04.759773Z","shell.execute_reply":"2025-09-13T12:21:00.954486Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/141k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08724ab4575b4e418c74102ba2a9a0c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d44484f3eee64430ad4f473da2db9220"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a71af0bb70740d0b3e4329762722d3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86bce9405595457c9d638a933acfb01f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/601 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d00900860594e108736694ea4faed84"}},"metadata":{}},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n2025-09-13 12:18:18.103454: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1757765898.334639      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1757765898.399156      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a006fd8a07b4fa68c634d953689eb7c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98eaa5791ec049f4a2aedc4eb1744a2f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e38d1c11a7aa4e9f90345df2b00a1c0b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/4.55G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce5d0bd777784ac89d56aa48769cd36c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66153eae565243c2a118b8599f6a2118"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"051b77e3909f44b2ad892fb032da64f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64357bcc1f674395ad28b0a559d71a56"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"\n# Simple generate_reply function\ndef generate_reply(user_message, max_new_tokens=150):\n    \"\"\"\n    Generate a response from Mistral 7B Instruct.\n\n    Args:\n        user_message (str): Input text from the user\n        max_new_tokens (int): Maximum number of tokens to generate\n\n    Returns:\n        str: Generated response\n    \"\"\"\n    # Prepare prompt for instruct model\n    prompt = f\"User: {user_message}\\nAssistant:\"\n\n    # Tokenize input\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n\n    # Generate output tokens\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            do_sample=True,\n            top_p=0.9,\n            temperature=0.7\n        )\n\n    # Decode and clean up\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    # Remove the prompt part from the response\n    reply = response[len(prompt):].strip()\n    return reply\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T12:21:09.692881Z","iopub.execute_input":"2025-09-13T12:21:09.693994Z","iopub.status.idle":"2025-09-13T12:21:09.698560Z","shell.execute_reply.started":"2025-09-13T12:21:09.693969Z","shell.execute_reply":"2025-09-13T12:21:09.697868Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"prompt = f\"<s>[INST] You are a database optimization assistant.\\n\" \\\n         \"Analyze the following query workload and recommend specific optimizations.\\n\" \\\n         \"Do not repeat the prompt\" \\\n         \"Focus on:\\n\" \\\n         \"1. Index usage or creation.\\n\" \\\n         \"2. Query rewrite opportunities (avoid SELECT *).\\n\" \\\n         \"3. Table partitioning or materialized views.\\n\" \\\n         \"4. Caching or other performance improvements.\\n\\n\" \\\n         f\"Query:\\n{contexts[0]}\\n\" \\\n         \"[/INST]\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\noutputs = model.generate(\n    **inputs,\n    max_new_tokens=600,\n    temperature=0.5,\n    top_p=0.9\n)\n\ndecoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(decoded)  # print first chunk to check\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T09:07:05.827641Z","iopub.execute_input":"2025-09-13T09:07:05.827910Z","iopub.status.idle":"2025-09-13T09:07:43.891464Z","shell.execute_reply.started":"2025-09-13T09:07:05.827891Z","shell.execute_reply":"2025-09-13T09:07:43.890663Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"You are a database optimization assistant.\nAnalyze the following query workload and recommend specific optimizations.\nDo not repeat the promptFocus on:\n1. Index usage or creation.\n2. Query rewrite opportunities (avoid SELECT *).\n3. Table partitioning or materialized views.\n4. Caching or other performance improvements.\n\nQuery:\nCategory: poor_selectivity\nQuery Template:\nselect * from orders where customer_id = ?;\n\nPerformance Stats:\n- Frequency: 2\n- Avg Exec Time: 190.0 ms\n- Avg Rows Examined: 9750\n\nTable Metadata:\nTable 'orders': 500000 rows, 820.5 MB, Indexes=['id', 'customer_id'], Constraints=['FOREIGN KEY', 'PRIMARY KEY'], Partitioned=False\n Based on the provided query workload, here are my recommendations for optimizing the performance:\n\n1. Index Usage or Creation:\n   Since the query is filtering on the `customer_id` column, which is indexed, it seems like the index is not being used effectively. The high average execution time and rows examined suggest that a covering index could help improve performance. A covering index includes all the columns needed for the query, so the table doesn't need to be scanned. In this case, you could create a covering index on `customer_id` including the necessary columns from the `orders` table.\n\n   ```sql\n   CREATE INDEX idx_orders_customer_id_covering\n   ON orders (customer_id, needed_columns)\n   ```\n\n2. Query Rewrite Opportunities:\n   Avoid using `SELECT *` and explicitly list the columns needed in the query. This reduces the amount of data that needs to be read from the disk and can improve performance.\n\n   ```sql\n   SELECT order_id, order_date, total_amount, ... FROM orders WHERE customer_id = ?;\n   ```\n\n3. Table Partitioning or Materialized Views:\n   Since the table is large (500,000 rows) and the query is filtering on a single column, table partitioning could be beneficial. Partitioning the `orders` table by `customer_id` would allow the database to read smaller, more manageable partitions instead of scanning the entire table.\n\n   ```sql\n   ALTER TABLE orders PARTITION BY RANGE (customer_id) (\n     PARTITION p0 VALUES LESS THAN (1000),\n     PARTITION p1 VALUES LESS THAN (2000),\n     ...\n     PARTITION pN VALUES LESS THAN MAXVALUE\n   );\n   ```\n\n4. Caching or Other Performance Improvements:\n   Caching can help reduce the number of disk reads and improve performance for frequently executed queries. You can configure your database system to cache the results of this query for a certain period. However, caching might not be necessary if the data changes frequently, as the cache would become outdated.\n\n   Additionally, consider optimizing database configuration settings such as buffer pool size, query cache size, and connection pool size to accommodate the workload.\n","output_type":"stream"}],"execution_count":13}]}